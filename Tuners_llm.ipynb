{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "from huggingface_hub import login\n",
    "notebook_login()\n",
    "login(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install git+https://github.com/huggingface/transformers \n",
    "!pip install  datasets loralib sentencepiece\n",
    "!pip install bitsandbytes accelerate xformers einops scipy\n",
    "!pip install langchain\n",
    "!pip install ipywidgets\n",
    "!pip install unstructured\n",
    "!pip install \"unstructured[docx]\"\n",
    "!pip install -i https://test.pypi.org/simple/ bitsandbytes\n",
    "!pip install accelerate\n",
    "!pip install sentence_transformers\n",
    "!pip install InstructorEmbedding\n",
    "!pip install chromadb\n",
    "!pip install langchainhub\n",
    "!pip install pypdf\n",
    "!pip install chroma-migrate\n",
    "!pip install pdfminer\n",
    "!pip install pdf2image\n",
    "!pip install pdfminer.six\n",
    "!pip install python-docx\n",
    "!pip install pillow-heif\n",
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain.document_loaders import UnstructuredFileLoader,PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import (UnstructuredFileLoader,UnstructuredPDFLoader,\n",
    "                                        PDFMinerPDFasHTMLLoader,BSHTMLLoader,\n",
    "                                    )\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "import shutil\n",
    "import chromadb\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.document_loaders import PDFMinerPDFasHTMLLoader\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains import ReduceDocumentsChain, MapReduceDocumentsChain\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "import torch, gc\n",
    "import transformers\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate, \n",
    "    MessagesPlaceholder, \n",
    "    SystemMessagePromptTemplate, \n",
    "    HumanMessagePromptTemplate\n",
    ")\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rajaguhan/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the SentenceTransformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2', device=\"cuda\")\n",
    "\n",
    "# Example sentence and instruction\n",
    "sentence = \"3D ActionSLAM: wearable person tracking in multi-floor environments\"\n",
    "instruction = \"Represent the Science title:\"\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = model.encode([f\"{instruction}: {sentence}\"])\n",
    "print(embeddings.shape)  # Check the dimensions of the embeddings\n",
    "\n",
    "# Set the directory for persisting Chroma database\n",
    "persist_directory = \"/home/rajaguhan/projects/chroma/\"\n",
    "\n",
    "# Initialize the embedding engine with the correct model name\n",
    "hf_embedding = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\", model_kwargs={\"device\": \"cuda\"})\n",
    "\n",
    "# Load the document\n",
    "file_path_name = \"/home/rajaguhan/projects/Taxation.txt\"\n",
    "loader = TextLoader(file_path_name)\n",
    "documents = loader.load()\n",
    "\n",
    "# Split the document into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100, separators=['\\n\\n', '\\n', ' ', ''])\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# Create the Chroma database from documents\n",
    "db = Chroma.from_documents(docs, hf_embedding, persist_directory=persist_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_html_loader(file_name):\n",
    "    loader = PDFMinerPDFasHTMLLoader(file_name)\n",
    "    data = loader.load()[0]   # entire PDF is loaded as a single Document\n",
    "    \n",
    "    soup = BeautifulSoup(data.page_content,'html.parser')\n",
    "    content = soup.find_all('div')\n",
    "    \n",
    "    cur_fs = None\n",
    "    cur_text = ''\n",
    "    snippets = []   # first collect all snippets that have the same font size\n",
    "    for c in content:\n",
    "        sp = c.find('span')\n",
    "        if not sp:\n",
    "            continue\n",
    "        st = sp.get('style')\n",
    "        if not st:\n",
    "            continue\n",
    "        fs = re.findall('font-size:(\\d+)px',st)\n",
    "        if not fs:\n",
    "            continue\n",
    "        fs = int(fs[0])\n",
    "        if not cur_fs:\n",
    "            cur_fs = fs\n",
    "        if fs == cur_fs:\n",
    "            cur_text += c.text\n",
    "        else:\n",
    "            snippets.append((cur_text,cur_fs))\n",
    "            cur_fs = fs\n",
    "            cur_text = c.text\n",
    "    snippets.append((cur_text,cur_fs))\n",
    "    # Note: The above logic is very straightforward. One can also add more strategies such as removing duplicate snippets (as\n",
    "    # headers/footers in a PDF appear on multiple pages so if we find duplicates it's safe to assume that it is redundant info)\n",
    "\n",
    "    cur_idx = -1\n",
    "    semantic_snippets = []\n",
    "    # Assumption: headings have higher font size than their respective content\n",
    "    for s in snippets:\n",
    "        # if current snippet's font size > previous section's heading => it is a new heading\n",
    "        if not semantic_snippets or s[1] > semantic_snippets[cur_idx].metadata['heading_font']:\n",
    "            metadata={'heading':s[0], 'content_font': 0, 'heading_font': s[1]}\n",
    "            metadata.update(data.metadata)\n",
    "            semantic_snippets.append(Document(page_content='',metadata=metadata))\n",
    "            cur_idx += 1\n",
    "            continue\n",
    "        \n",
    "        # if current snippet's font size <= previous section's content => content belongs to the same section (one can also create\n",
    "        # a tree like structure for sub sections if needed but that may require some more thinking and may be data specific)\n",
    "        if not semantic_snippets[cur_idx].metadata['content_font'] or s[1] <= semantic_snippets[cur_idx].metadata['content_font']:\n",
    "            semantic_snippets[cur_idx].page_content += s[0]\n",
    "            semantic_snippets[cur_idx].metadata['content_font'] = max(s[1], semantic_snippets[cur_idx].metadata['content_font'])\n",
    "            continue\n",
    "        \n",
    "        # if current snippet's font size > previous section's content but less than previous section's heading than also make a new \n",
    "        # section (e.g. title of a PDF will have the highest font size but we don't want it to subsume all sections)\n",
    "        metadata={'heading':s[0], 'content_font': 0, 'heading_font': s[1]}\n",
    "        metadata.update(data.metadata)\n",
    "        semantic_snippets.append(Document(page_content='',metadata=metadata))\n",
    "        cur_idx += 1\n",
    "        \n",
    "    return semantic_snippets\n",
    "\n",
    "class Interface():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.set_db()\n",
    "        self.gpu_clear()\n",
    "        self.set_llm()\n",
    "        #self.set_llm_cpu()\n",
    "        self.convo_chain = self.convo_chain()\n",
    "        self.retreive_chain = self.retrieval_chain(db)\n",
    "        self.doc_chain = self.summarisation()\n",
    "        \n",
    "        self.memory = []\n",
    "        self.file_paths = []\n",
    "        self.a = 0\n",
    "        \n",
    "    def csv_loader(self, file_name):\n",
    "        loader = CSVLoader(file_path=file_name)\n",
    "        data = loader.load()\n",
    "\n",
    "        return data\n",
    "\n",
    "    def txt_loader(self, file_name):\n",
    "        loader = UnstructuredFileLoader(file_name)\n",
    "        documents = loader.load()\n",
    "\n",
    "        return documents\n",
    "\n",
    "    def pdf_loader(self, file_name):\n",
    "        loader = UnstructuredPDFLoader(file_name, mode=\"elements\")\n",
    "        data = loader.load()\n",
    "    \n",
    "        return data\n",
    "\n",
    "        \n",
    "    def html_loader(self, file_name):\n",
    "        loader = BSHTMLLoader(file_name)\n",
    "        data = loader.load()\n",
    "    \n",
    "        return data\n",
    "    \n",
    "    def mp4_loader(self, file_name):\n",
    "        pass\n",
    "        \n",
    "        #return data\n",
    "    \n",
    "    def mp3_loader(self, filename):\n",
    "        pass\n",
    "    \n",
    "        #return data\n",
    "        \n",
    "    def set_db(self):\n",
    "        \n",
    "        # embedding engine\n",
    "        self.persist_directory=\"/home/rajaguhan/projects/chroma\"\n",
    "        client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "        #self.embeddings = HuggingFaceInstructEmbeddings()\n",
    "        self.embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\",\n",
    "                                                        model_kwargs={\"device\":\"cuda\"})\n",
    "        \n",
    "    def use_db(self, documents):\n",
    "    \n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=3900, chunk_overlap=200, separators=[\"\\n\\n\", \"\\n\", \".\", \",\", \" \",\"\"])\n",
    "        docs = text_splitter.split_documents(documents)\n",
    "        \n",
    "        metadaata_list = list(docs[0].metadata.keys())\n",
    "        content = []\n",
    "        id = []\n",
    "        metadata = []    \n",
    "\n",
    "        for i,j in enumerate(docs):\n",
    "            d = {}\n",
    "            if \"page_number\"  in metadaata_list:\n",
    "                d[\"page_number\"] = j.metadata[\"page_number\"]\n",
    "            elif \"pagenumber\"  in metadaata_list:\n",
    "                d[\"page_number\"] = j.metadata[\"pagenumber\"]\n",
    "    \n",
    "            d[\"source\"] = j.metadata[\"source\"]\n",
    "    \n",
    "            content.append(j.page_content)\n",
    "            id.append(str(i))\n",
    "            metadata.append(d)\n",
    "            \n",
    "            j.metadata = d\n",
    "                    \n",
    "        self.db = Chroma.from_documents(\n",
    "                           embedding=self.embeddings,\n",
    "                           documents=docs, \n",
    "                           persist_directory=self.persist_directory, \n",
    "                           ids=id,  \n",
    "                        )\n",
    "        return self.db\n",
    "        #db.persist()\n",
    "        \n",
    "        return self.db\n",
    "    \n",
    "    def gpu_clear(self):\n",
    "        \n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()\n",
    "\n",
    "    def file_type(self, fname):\n",
    "        if fname[-4:]==\".txt\":\n",
    "            return self.txt_loader(fname)\n",
    "        elif fname[-4:]==\".pdf\":\n",
    "            return self.pdf_loader(fname)\n",
    "        elif fname[-4:]=='.csv':\n",
    "            return self.csv_loader(fname)\n",
    "        elif fname[-5:]==\".html\":\n",
    "            return self.html_loader(fname)\n",
    "        elif fname[-6:]==\".docx\":\n",
    "            return self.html_loader(fname)\n",
    "        elif fname[-4]==\".mp4\":\n",
    "            return self.mp4_loader(fname)\n",
    "        elif fname[-4]==\".mp3\":\n",
    "            return self.mp3_loader(fname)\n",
    "    \n",
    "    def upload_file(self, files):\n",
    "    \n",
    "        for file in files:\n",
    "            self.file_paths.append(file.name) \n",
    "        print(self.file_paths)\n",
    "        try:\n",
    "            shutil.move(self.file_paths[-1], \"/home/rajaguhan/projects\")\n",
    "        except:\n",
    "            print(\"File Already exists\")\n",
    "        \n",
    "        documents = self.file_type(self.file_paths[-1])\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=3900, chunk_overlap=200, separators=[\"\\n\\n\", \"\\n\", \".\", \",\", \" \",\"\"])\n",
    "        docs = text_splitter.split_documents(documents)\n",
    "        self.db = self.use_db(docs)\n",
    "        if self.a==0:\n",
    "            self.retreive_chain = self.retrieval_chain(self.db)\n",
    "            self.a+=1\n",
    "        doc_sum = self.doc_chain.run(docs)\n",
    "\n",
    "        return self.file_paths, doc_sum\n",
    "\n",
    "        \n",
    "    def set_llm(self, model_name=\"/home/rajaguhan/LLM_Models/llama-2-7b-chat.Q4_K_M.gguf\", n_gpu_layers=30, n_batch=512):\n",
    "        \n",
    "        callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "                            \n",
    "        self.llm = LlamaCpp(\n",
    "            model_path=model_name,\n",
    "            n_gpu_layers=n_gpu_layers, # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "            n_batch=n_batch,           # Change this value based on your model and your GPU VRAM pool.\n",
    "            callback_manager=callback_manager,\n",
    "            verbose=True,              # Verbose is required to pass to the callback manager\n",
    "            n_ctx=4096,\n",
    "            use_mlock=False,\n",
    "            use_mmap=True,\n",
    "            n_threads=16,\n",
    "            temperature=0.0\n",
    "        )\n",
    "        \n",
    "        return self.llm\n",
    "    \n",
    "    def set_llm_cpu(self, model_name=\"/home/rajaguhan/LLM_Models/llama-2-7b-chat.Q4_K_M.gguf\"):\n",
    "        \n",
    "        callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "                            \n",
    "        self.llm2 = LlamaCpp(\n",
    "            model_path=model_name,\n",
    "            callback_manager=callback_manager,\n",
    "            verbose=True,              # Verbose is required to pass to the callback manager\n",
    "            n_ctx=4096,\n",
    "            use_mlock=False,\n",
    "            use_mmap=True,\n",
    "            n_threads=16,\n",
    "            temperature=0.0\n",
    "        )\n",
    "        \n",
    "        return self.llm2\n",
    "    \n",
    "    def convo_chain(self):\n",
    "        \n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            SystemMessagePromptTemplate.from_template(\"\"\"[INST]<>\n",
    "                In your role as a sentence reformatter, your task is to reformat sentences, making them comprehensible even without the context of chat history.\n",
    "                The reformatting should maintain a similar word count to the user's input, ensuring clarity and understanding.\n",
    "                It is important not to provide an answer to the original sentence, but to focus solely on the reformatting process. Please reformat the message as mentioned above.\n",
    "                <>\"\"\"),\n",
    "            MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "            HumanMessagePromptTemplate.from_template(template=\"{input}\")\n",
    "        ])\n",
    "\n",
    "        self.memory1 = ConversationBufferMemory(return_messages=True,memory_key=\"chat_history\")\n",
    "        self.conversation = ConversationChain(memory=self.memory1, prompt=prompt, llm=self.llm,verbose=True)\n",
    "        \n",
    "        return self.conversation\n",
    "    \n",
    "    def retrieval_chain(self,db):\n",
    "        \n",
    "        \n",
    "        SYSTEM_PROMPT = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "            If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "            Keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer.\"\"\"\n",
    "\n",
    "        B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "        B_SYS, E_SYS = \"<>\\n\", \"\\n<>\\n\\n\"\n",
    "        \n",
    "        SYSTEM_PROMPT = B_SYS + SYSTEM_PROMPT + E_SYS\n",
    "        \n",
    "        instruction = \"\"\"\n",
    "        {context}\n",
    "        \n",
    "        Question: {question}\n",
    "        \"\"\"\n",
    "        template = B_INST + SYSTEM_PROMPT + instruction + E_INST\n",
    "             \n",
    "        QA_CHAIN_PROMPT = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
    "        \n",
    "        self.qa = RetrievalQA.from_chain_type(\n",
    "                                llm=self.llm, \n",
    "                                chain_type=\"stuff\", \n",
    "                                retriever=db.as_retriever(search_type = \"mmr\",kwargs={\"k\":3,\"fetch_k\":6}),\n",
    "                                #return_source_documents=True,\n",
    "                                chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
    "                                verbose=True,\n",
    "                            )\n",
    "        return self.qa\n",
    "    \n",
    "    def summarisation(self):\n",
    "\n",
    "        map_template = \"\"\"You are an extraordinary agent with a specialization in summarizing complex science, AMP, law, mathematics, and technology documents. Your expertise lies in condensing extensive volumes of information into concise and comprehensive summaries, while ensuring that the key crux components and ideas are preserved.\\n\n",
    "        The following is a set of pages of a document\n",
    "        {docs}\n",
    "        Based on this documents content and infomation, please identify the main themes and summarize it with without leaving any key info and vital information.\\n  \n",
    "        Helpful Answer:\"\"\"\n",
    "        map_prompt = PromptTemplate.from_template(map_template)\n",
    "        map_chain = LLMChain(llm=self.llm, prompt=map_prompt)\n",
    "\n",
    "        reduce_template = \"\"\"You are an extraordinary agent with a specialization in summarizing complex science, AMP, law, mathematics, and technology documents. Your expertise lies in condensing extensive volumes of information into concise and comprehensive summaries, while ensuring that the key crux components and ideas are preserved.\\n\n",
    "        The following is set of summaries:\n",
    "        {doc_summaries}\n",
    "        Take these and distill it into a final, consolidated summary of the main themes which consist of all the key info and vital information. \n",
    "        Helpful Answer:\"\"\"\n",
    "        reduce_prompt = PromptTemplate.from_template(reduce_template)\n",
    "        reduce_chain = LLMChain(llm=self.llm, prompt=reduce_prompt)\n",
    "\n",
    "        combine_documents_chain = StuffDocumentsChain(\n",
    "            llm_chain=reduce_chain, document_variable_name=\"doc_summaries\"\n",
    "        )\n",
    "\n",
    "        reduce_documents_chain = ReduceDocumentsChain(\n",
    "            combine_documents_chain=combine_documents_chain,\n",
    "            collapse_documents_chain=combine_documents_chain,\n",
    "            token_max=4000,\n",
    "        )\n",
    "\n",
    "        self.map_reduce_chain = MapReduceDocumentsChain(\n",
    "            llm_chain=map_chain,\n",
    "            reduce_documents_chain=reduce_documents_chain,\n",
    "            document_variable_name=\"docs\",\n",
    "            return_intermediate_steps=False,\n",
    "        )\n",
    "        \n",
    "        return self.map_reduce_chain \n",
    "    \n",
    "    def grammatical_and_textsum(self, which, content):\n",
    "        prompt = PromptTemplate(\n",
    "            template=\"\"\"{which}\n",
    "                \"{content}\".\"\"\",\n",
    "            input_variables=[\"which\", \"content\"]\n",
    "        )\n",
    "        prompt_formatted = prompt.format(which=which, content=content)\n",
    "        gram_or_textsum = LLMChain(prompt=prompt,llm=self.llm, verbose=True)\n",
    "        out = gram_or_textsum.predict(which=which, content=content)\n",
    "        return out\n",
    "        \n",
    "\n",
    "    def echo_chat(self,msg, history):\n",
    "        query = self.convo_chain.run(msg)\n",
    "        crt_query = query.split(\":\")[-1]\n",
    "        #crt_query = msg\n",
    "        answer = self.retreive_chain.run(str(crt_query))\n",
    "        self.memory.save_context({\"Human\": crt_query}, {\"AI\": answer})\n",
    "        \n",
    "        self.memory.append([\"Human\",msg])\n",
    "        self.memory.append([\"AI\",answer])\n",
    "        \n",
    "        return self.memory[-1][0], self.memory[-1][1]\n",
    "\n",
    "    \n",
    "    def respond(self, msg, txt_input=\"\", choice=\"Question Answering\"):\n",
    "        print(choice, msg)\n",
    "        s = \"\"  # Assign a default value to 's'\n",
    "        if choice == \"Question Answering\":\n",
    "            print(msg)\n",
    "            crt_query = msg\n",
    "            answer = self.retreive_chain.run(str(crt_query))\n",
    "            self.memory1.save_context({\"Human\": crt_query}, {\"AI\": answer})\n",
    "\n",
    "            self.memory.append([\"Human\", msg])\n",
    "            self.memory.append([\"AI\", answer])\n",
    "            s = answer\n",
    "        elif choice == \"Summarizer\" or choice == \"Grammatical Error Checker\":\n",
    "            if choice == \"Summarizer\":\n",
    "                prompt = \"You are an exceptional expert in summarizing paragraphs on a wide range of subjects including science and AMP, mathematics, socials, philosophy, and technology. Your expertise lies in extracting the main information and important points from any number of paragraphs without any significant omissions. And It should be in the form of bullet points.\"\n",
    "                s = self.grammatical_and_textsum(which=prompt, content=txt_input)\n",
    "            else:\n",
    "                prompt = \"Your area of expertise includes English philosophy, grammar, and vocabulary. Your primary responsibility is to identify and correct any grammatical mistakes in the provided set of paragraphs, ensuring accuracy and clarity of the content.\"\n",
    "                s = self.grammatical_and_textsum(which=prompt, content=txt_input)\n",
    "            \n",
    "        return msg, s\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from /home/rajaguhan/LLM_Models/llama-2-7b-chat.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens cache size = 259\n",
      "llm_load_vocab: token to piece cache size = 0.1684 MB\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.80 GiB (4.84 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3060 Laptop GPU, compute capability 8.6, VMM: yes\n",
      "llm_load_tensors: ggml ctx size =    0.30 MiB\n",
      "llm_load_tensors: offloading 30 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 30/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  3891.24 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size =  3470.77 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:  CUDA_Host KV buffer size =   128.00 MiB\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =  1920.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   353.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =    24.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 26\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '32', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
      "Using fallback chat format: llama-2\n",
      "/home/rajaguhan/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "X = Interface()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://70a739f9f2363a165c.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://70a739f9f2363a165c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/tmp/gradio/e39661ee7479abe4033d2e311c72a98e586e27f3/indian_taxation_system.txt']\n",
      "File Already exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rajaguhan/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "The Indian Taxation System has a long history dating back to ancient times, with modern taxation established during the British colonial period. The system is broadly divided into direct and indirect taxes, with GST being the most recent reform. Key provisions include filing tax returns, tax deductions and exemptions, tax audit, advance tax, and self-assessment tax. Tax planning and management involve investment options, tax-saving instruments, and compliance measures. Recent developments have focused on improving compliance, reducing evasion, and increasing the ease of doing business. Future reforms may include further simplification of tax laws, digitization of tax administration, and broadening the tax base."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  124123.80 ms\n",
      "llama_print_timings:      sample time =      52.40 ms /   153 runs   (    0.34 ms per token,  2919.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =  168340.20 ms /   949 tokens (  177.39 ms per token,     5.64 tokens per second)\n",
      "llama_print_timings:        eval time =  178230.27 ms /   152 runs   ( 1172.57 ms per token,     0.85 tokens per second)\n",
      "llama_print_timings:       total time =  347182.23 ms /  1101 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The Indian Taxation System has a long history with modern taxation established during British colonial period. The system is divided into direct and indirect taxes, with GST being most recent reform. Key provisions include filing tax returns, tax deductions and exemptions, tax audit, advance tax, and self-assessment tax. Tax planning and management involve investment options, tax-saving instruments, and compliance measures. Recent developments have focused on improving compliance, reducing evasion, and increasing ease of doing business. Future reforms may include simplification of tax laws, digitization of tax administration, and broadening tax base.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  124123.80 ms\n",
      "llama_print_timings:      sample time =      48.18 ms /   142 runs   (    0.34 ms per token,  2947.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12649.77 ms /   197 tokens (   64.21 ms per token,    15.57 tokens per second)\n",
      "llama_print_timings:        eval time =   80689.56 ms /   141 runs   (  572.27 ms per token,     1.75 tokens per second)\n",
      "llama_print_timings:       total time =   93873.11 ms /   338 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question Answering Mention 3 Important Taxation Laws of India in bullet points\n",
      "Mention 3 Important Taxation Laws of India in bullet points\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Thanks for asking! Here are three important taxation laws in India:\n",
      "1. Income Tax Act, 1961 - This law governs the taxation of income of individuals and businesses in India. It provides for various tax slabs, deductions, and exemptions to reduce taxable income.\n",
      "2. Goods and Services Tax (GST) Act, 2017 - GST is a comprehensive indirect tax on the manufacture, sale, and consumption of goods and services throughout India. It replaced several other indirect taxes, such as excise duty, service tax, and value-added tax (VAT).\n",
      "3. Customs Act, 1962 - This law governs the levy of customs duties on goods imported into India. It provides for various types of customs duties, including basic customs duty, additional customs duty, and countervailing duty."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  124123.80 ms\n",
      "llama_print_timings:      sample time =      68.24 ms /   195 runs   (    0.35 ms per token,  2857.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =  174802.66 ms /  1587 tokens (  110.15 ms per token,     9.08 tokens per second)\n",
      "llama_print_timings:        eval time =  317930.48 ms /   194 runs   ( 1638.82 ms per token,     0.61 tokens per second)\n",
      "llama_print_timings:       total time =  493582.45 ms /  1781 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "with gr.Blocks( \n",
    "    title=\"TUNERS.AI\",\n",
    "    theme=\"gradio/soft\",\n",
    "    ) as demo:\n",
    "    gr.Markdown(\n",
    "    \"\"\"\n",
    "    # TUNERS.AI  -  AI Assistant which can Summarize and Answer any question on the uploaded Documents . \n",
    "    \"\"\")\n",
    "    with gr.Row():\n",
    "        choice = gr.Dropdown(label=\"Choose TASK (Summarizer OR Grammatical Error Checker OR Q & A)\",choices=[\"Summarizer\",\"Grammatical Error Correcter\", \"Question Answering\"],)\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            txt_input = gr.Textbox(label=\"INPUT\", lines=6, value=\"Input - Text goes here...\", scale=5)\n",
    "        with gr.Column():\n",
    "            txt_out = gr.Textbox(label=\"RESULT\", lines=6, value=\"Output - Text comes here...\", scale=5)\n",
    "    with gr.Row():\n",
    "        msg = gr.Textbox(label=\"Prompt Input....Ask Questions related to the Uploaded Docs...\",lines=3, value=\"Summazrize the all the content of the docs\",scale=5)\n",
    "    with gr.Row():\n",
    "        doc_sum = gr.Textbox(label=\"Summaraized Content from Docs...\",lines=3, value=\"\",scale=5)\n",
    "    with gr.Row():\n",
    "        file_output = gr.File()\n",
    "        upload_button = gr.UploadButton(file_types=['pdf','text',\"audio\", \"video\"], file_count=\"multiple\", size='sm',scale=1)\n",
    "        upload_button.upload(X.upload_file, upload_button, outputs=[file_output, doc_sum])\n",
    "\n",
    "    btn_sub = gr.Button(\"Submit\")\n",
    "    clear = gr.ClearButton(components=[msg], value=\"Clear console\")\n",
    "    btn_reset = gr.Button(\"Reset\")\n",
    "\n",
    "    btn_sub.click(X.respond, inputs=[msg, txt_input, choice], outputs=[msg, txt_out])\n",
    "    msg.submit(X.respond, inputs=[msg, txt_input, choice], outputs=[msg, txt_out]) #Press enter to submit\n",
    "gr.close_all()\n",
    "demo.queue().launch(share=True,debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
